{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99391969-0fa0-465d-ac75-6456135eb46a",
   "metadata": {},
   "source": [
    "# Fine-Tuning for Amharic Named Entity Recognition (NER) from Telegram Data\n",
    "* To fine-tune an NER model for Amharic using your Telegram data, here's a step-by-step approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a95b03-2fe9-4f9f-bbbd-8300325f74bd",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "* First, we'll need to extract and structure the relevant information from our Excel data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70741a2-5540-4d9d-abd5-ae1590762f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('telegram_data.xlsx')\n",
    "\n",
    "# Extract messages with Amharic text\n",
    "amharic_messages = df[df['Message'].notna()]['Message'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00342a67-30cd-4086-8f79-f1b58afcc409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel Title</th>\n",
       "      <th>Channel Username</th>\n",
       "      <th>ID</th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "      <th>Media Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-20 11:50:03+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5333.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-20 11:50:03+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5332.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-20 11:50:03+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5331.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-20 11:50:02+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5330.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-20 11:50:02+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5329.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Channel Title    Channel Username    ID Message  \\\n",
       "0  Sheger online-store  @Shageronlinestore  5333     NaN   \n",
       "1  Sheger online-store  @Shageronlinestore  5332     NaN   \n",
       "2  Sheger online-store  @Shageronlinestore  5331     NaN   \n",
       "3  Sheger online-store  @Shageronlinestore  5330     NaN   \n",
       "4  Sheger online-store  @Shageronlinestore  5329     NaN   \n",
       "\n",
       "                       Date                          Media Path  \n",
       "0 2024-09-20 11:50:03+00:00  photos/@Shageronlinestore_5333.jpg  \n",
       "1 2024-09-20 11:50:03+00:00  photos/@Shageronlinestore_5332.jpg  \n",
       "2 2024-09-20 11:50:03+00:00  photos/@Shageronlinestore_5331.jpg  \n",
       "3 2024-09-20 11:50:02+00:00  photos/@Shageronlinestore_5330.jpg  \n",
       "4 2024-09-20 11:50:02+00:00  photos/@Shageronlinestore_5329.jpg  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ff000-fd23-4a36-b171-688d29d55c84",
   "metadata": {},
   "source": [
    "### Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d2f250-ede2-407b-a2d7-b651cb08fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5015 entries, 0 to 5014\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   Channel Title     5015 non-null   object             \n",
      " 1   Channel Username  5015 non-null   object             \n",
      " 2   ID                5015 non-null   int64              \n",
      " 3   Message           3166 non-null   object             \n",
      " 4   Date              5015 non-null   datetime64[ns, UTC]\n",
      " 5   Media Path        3794 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(1), object(4)\n",
      "memory usage: 235.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acaf2f2-6d5f-4855-bb81-e8adc82b97e4",
   "metadata": {},
   "source": [
    "###  Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e69476-7d4f-45a2-9e16-f76803884237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel Title          0\n",
       "Channel Username       0\n",
       "ID                     0\n",
       "Message             1849\n",
       "Date                   0\n",
       "Media Path          1221\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef5322d-6e65-4410-a13f-108c25c38c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977b4f8-18fe-48a1-81e9-d7bc28ef2463",
   "metadata": {},
   "source": [
    "### Droping Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0961aeea-faaf-43da-a638-4ff70e5428d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where both Message and Media Path are missing.\n",
    "df = df.dropna(subset=['Message'], how='all')\n",
    "df = df.dropna(subset=['Media Path'], how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73f94c93-5ab5-476f-8481-efb9d8a065a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel Title       0\n",
       "Channel Username    0\n",
       "ID                  0\n",
       "Message             0\n",
       "Date                0\n",
       "Media Path          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25992d24-80b4-4aef-9eb1-9d574d4e19b5",
   "metadata": {},
   "source": [
    "# 2. Annotation Guidelines\n",
    "## Define our entity types based on what appears in our data:\n",
    "\n",
    "* Products (e.g., \"ሴራሚክ የላዛኛ መስሪያ\", \"የፀጉር ፔስትራ\")\n",
    "* Prices (e.g., \"1200 ብር\", \"500 ብር\")\n",
    "* Locations/Addresses (e.g., \"ስሪ ኤም ሲቲ ሞል ሁለተኛ ፎቅ\")\n",
    "* Phone numbers\n",
    "* Measurements (e.g., \"3ሜትር\", \"45ሳ.ሜ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a8563-905d-40a8-83a3-70cb1d549c75",
   "metadata": {},
   "source": [
    "# 3. Data Annotation\n",
    "* We'll need to annotate our data in BIO format.\n",
    "### For annotation,We can use tools like:\n",
    "* Doccano (open source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2fe3d2-adbb-4483-8e6d-987564b41279",
   "metadata": {},
   "source": [
    "### Preprocessing Recomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea9969-e4ba-468e-9446-fd8de95fa2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_amharic_text(text):\n",
    "    # Normalize Ethiopic numbers if present\n",
    "    text = text.replace('፩', '1').replace('፪', '2') # etc for all Ethiopic numbers\n",
    "    \n",
    "    # Standardize price formats\n",
    "    text = re.sub(r'(\\d+)\\s*ብር', r'\\1 ብር', text)\n",
    "    \n",
    "    # Remove excessive whitespace and line breaks\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de497459-474c-4509-911e-bd2f70be187e",
   "metadata": {},
   "source": [
    "# 4. Model Selection\n",
    "### For Amharic NER, consider these options:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a20c5-57de-40ca-95f1-87410c9fbd6f",
   "metadata": {},
   "source": [
    "## Option A: Use AfroXLMRoberta (better for African languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64ab45d9-b50c-4997-a2a8-07458ce2ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Davlan/afro-xlmr-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb05cb3-ebda-4e81-ba46-54553af74bcd",
   "metadata": {},
   "source": [
    "# 5. Training Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b2cfe3-8e1e-4340-ad19-bdaf75fd5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']\n",
    "num_labels = len(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf37256-611b-4edf-9456-92792cf60230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a158be8cf54f8e813705dfa27ff32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dell\\.cache\\huggingface\\hub\\models--Davlan--afro-xlmr-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ff98cb178148bca6d9c1931a6e88ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/de/c1/dec148c5602335191b2135d8b3035439e0f79c61b35648cb15f8a50abb293ef6/f73b04316c9c2e76fcb213957616a09e1eb44112266717f649fc1897dac42953?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1750648405&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDY0ODQwNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9kZS9jMS9kZWMxNDhjNTYwMjMzNTE5MWIyMTM1ZDhiMzAzNTQzOWUwZjc5YzYxYjM1NjQ4Y2IxNWY4YTUwYWJiMjkzZWY2L2Y3M2IwNDMxNmM5YzJlNzZmY2IyMTM5NTc2MTZhMDllMWViNDQxMTIyNjY3MTdmNjQ5ZmMxODk3ZGFjNDI5NTM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jWnXIgLuzTA57ByYbTkrtcKc4ndG%7E3P6JAlJQRxB5sWjX6KcM4BQQJANmffX4y9JC9DK-L5YaU1QFZYUjH%7El69goeJ4zBqMmALfLpOoGa4fTOVfQ8MNMiZEP21Dqt5CHj%7EVxwZSnhFzUkiNt0q0Tt996kpYBIQzrBWq8HReW2JxdQjSlejJljKQjmtHwHasl3-piuSwtUEzC2WreVxIeGRTnUEsODJr%7EGz2WktD1-I817L9qxEx1%7EFdRtXUkSloTB3gq6hnqWxg1emZ%7Exf%7En1AGPBsu1WC7UJZmx-w4ko4fEEP2DTIp0idKxPRSeWPPj4%7EA8C-fSEuiO2EWrTWGvsw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ac2528ce1846b3bf4f8df7573fd0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  42%|####2     | 818M/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# STEP 1: Define your label list\n",
    "label_list = ['O', 'B-Product', 'I-Product', 'B-PRICE', 'I-PRICE', 'B-LOC', 'I-LOC']\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# STEP 2: Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/afro-xlmr-base\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/afro-xlmr-base\", num_labels=num_labels)\n",
    "\n",
    "# STEP 3: Load and prepare your CoNLL data\n",
    "# Example: read your conll file (replace with your file path)\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def read_conll_data(conll_file):\n",
    "    with open(conll_file, encoding=\"utf-8\") as f:\n",
    "        tokens, labels, data = [], [], []\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                if tokens:\n",
    "                    data.append({\"tokens\": tokens, \"ner_tags\": [label_to_id[l] for l in labels]})\n",
    "                    tokens, labels = [], []\n",
    "            else:\n",
    "                token, label = line.strip().split()\n",
    "                tokens.append(token)\n",
    "                labels.append(label)\n",
    "        if tokens:\n",
    "            data.append({\"tokens\": tokens, \"ner_tags\": [label_to_id[l] for l in labels]})\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "dataset = read_conll_data(\"ner_data.conll\")\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test[\"train\"]\n",
    "eval_dataset = train_test[\"test\"]\n",
    "\n",
    "# STEP 4: Tokenize and align labels\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# STEP 5: Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# STEP 6: Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# STEP 7: Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0b022-3ebd-42f5-8cdd-f135f36bb5fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1adaf4-9f2f-41aa-97e5-9bb46fff0175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
